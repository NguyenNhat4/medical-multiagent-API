# Core framework import
from pocketflow import Node

# Standard library imports
import logging

# Configure logging for this module with Vietnam timezone
from utils.timezone_utils import setup_vietnam_logging
from config.logging_config import logging_config

if logging_config.USE_VIETNAM_TIMEZONE:
    logger = setup_vietnam_logging(__name__, 
                                 level=getattr(logging, logging_config.LOG_LEVEL.upper()),
                                 format_str=logging_config.LOG_FORMAT)
else:
    logger = logging.getLogger(__name__)
    logger.setLevel(getattr(logging, logging_config.LOG_LEVEL.upper()))


# ========== Medical Agent Nodes ==========

class IngestQuery(Node):
    def prep(self, shared):
        logger.info("üîç [IngestQuery] PREP - ƒê·ªçc role v√† input t·ª´ shared")
        role = shared.get("role", "")
        user_input = shared.get("input", "")
        logger.info(f"üîç [IngestQuery] PREP - Role: {role}, Users Input : {user_input}")
        return role, user_input

    def exec(self, inputs):
        logger.info("üîç [IngestQuery] EXEC - X·ª≠ l√Ω role v√† query")
        role, user_input = inputs
        result = {"role": role, "query": user_input.strip()}
        logger.info(f"üîç [IngestQuery] EXEC - Processed: {result}")
        return result

    def post(self, shared, prep_res, exec_res):
        logger.info("üîç [IngestQuery] POST - L∆∞u role v√† query v√†o shared")
        shared["role"] = exec_res["role"]
        shared["query"] = exec_res["query"]
        logger.info(f"üîç [IngestQuery] POST - Saved role: {exec_res['role']}, query: {exec_res['query'][:50]}...")
        return "default"

class RagAgent(Node):
    """
    RAG Agent - intelligent decision maker that orchestrates the retrieval pipeline:
    1. Decide if we need to classify topic (get DEMUC, CHU_DE_CON)
    2. Decide if we need to expand query
    3. Trigger retrieval when ready
    4. Route to compose answer after retrieval
    
    State machine:
    - init -> classify (if no metadata) -> expand (if needed) -> retrieve -> compose_answer
    """

    def prep(self, shared):
        logger.info("ü§ñ [RagAgent] PREP - Analyzing current state and making decision")
        query = shared.get("query", "")
        user_role = shared.get("role", "")
        demuc = shared.get("demuc", "")
        chu_de_con = shared.get("chu_de_con", "")
        rag_state = shared.get("rag_state", "init")
        retrieved_candidates = shared.get("retrieved_candidates", [])
        selected_ids = shared.get("selected_ids", [])
        expansion_tried = shared.get("expansion_tried", False)
        retrieve_attempts = shared.get("retrieve_attempts", 0)

        # Load filtered questions (selected by FilterAgent)
        filtered_questions = []
        if selected_ids and retrieved_candidates:
            # Map selected IDs to actual questions
            candidate_map = {c["id"]: c["CAUHOI"] for c in retrieved_candidates}
            filtered_questions = [
                {"id": qid, "question": candidate_map.get(qid, "")}
                for qid in selected_ids
                if qid in candidate_map
            ]

        logger.info(f"ü§ñ [RagAgent] PREP - state='{rag_state}', query='{query[:50]}...', {len(filtered_questions)} filtered questions, attempts={retrieve_attempts}")
        return query, user_role, demuc, chu_de_con, rag_state, filtered_questions, expansion_tried, retrieve_attempts

    def exec(self, inputs):
        from utils.llm import call_llm
        from utils.parsing import parse_yaml_with_schema
        from utils.auth import APIOverloadException
        from config.timeout_config import timeout_config

        query, user_role, demuc, chu_de_con, rag_state, filtered_questions, expansion_tried, retrieve_attempts = inputs
        logger.info(f"ü§ñ [RagAgent] EXEC - Current state: {rag_state}, {len(filtered_questions)} questions, attempts: {retrieve_attempts}")

        # Format filtered questions for LLM
        questions_str = ""
        if filtered_questions:
            questions_str = "\n".join([
                f"{i}. {q['question'][:80]}..." if len(q['question']) > 80 else f"{i}. {q['question']}"
                for i, q in enumerate(filtered_questions, 1)
            ])

        # Build context
        context = f"""Query: "{query}"
Metadata: DEMUC="{demuc}", CHU_DE_CON="{chu_de_con}"
State: {rag_state}
Retrieve attempts: {retrieve_attempts}/2

Filtered questions ({len(filtered_questions)}):
{questions_str if questions_str else "(none)"}"""


        prompt = f"""RAG Agent quy·∫øt ƒë·ªãnh b∆∞·ªõc ti·∫øp.

{context}

Actions:
- retry_retrieve: Th·ª≠ l·∫°i retrieval
- compose_answer: So·∫°n tr·∫£ l·ªùi

Rules:
1. N·∫øu attempts >= 2 ‚Üí B·∫ÆT BU·ªòC compose_answer (ƒë√£ h·∫øt l∆∞·ª£t retry)
2. N·∫øu c√≥ ƒë·ªß c√¢u h·ªèi (‚â• 2) ‚Üí compose_answer
3. N·∫øu kh√¥ng c√≥ c√¢u h·ªèi + attempts < 2 ‚Üí retry_retrieve

YAML:
```yaml
next_action: "..."
reason: "..."
```"""

        try:
            resp = call_llm(prompt, fast_mode=True, max_retry_time=timeout_config.LLM_RETRY_TIMEOUT)

            result = parse_yaml_with_schema(
                resp,
                required_fields=["next_action", "reason"],
                field_types={"next_action": str, "reason": str}
            )

            # Validate action
            valid_actions = ["retry_retrieve", "compose_answer"]
            if result["next_action"] not in valid_actions:
                raise ValueError(f"Invalid action: {result['next_action']}")

            logger.info(f"ü§ñ [RagAgent] Decision: {result['next_action']} - {result['reason']}")
            return result

        except APIOverloadException:
            logger.error("ü§ñ [RagAgent] API overloaded")
            raise
        except Exception as e:
            logger.error(f"ü§ñ [RagAgent] Error: {e}")
            raise

    def post(self, shared, prep_res, exec_res):
        next_action = exec_res["next_action"]
        reason = exec_res.get("reason", "")
        current_attempts = shared.get("retrieve_attempts", 0)

        logger.info(f"ü§ñ [RagAgent] POST - Next action: '{next_action}' | Reason: {reason} | Current attempts: {current_attempts}")

        # Update state based on next action
        if next_action == "retry_retrieve":
            # Increment retrieve attempts counter
            shared["retrieve_attempts"] = current_attempts + 1
            shared["rag_state"] = "init"  # Reset to init for retrieve_flow to start fresh
            logger.info(f"ü§ñ [RagAgent] POST - Retrying retrieval pipeline (attempt {current_attempts + 1}/2)")
            return "retry_retrieve"
        elif next_action == "compose_answer":
            shared["rag_state"] = "composing"
            logger.info("ü§ñ [RagAgent] POST - Proceeding to compose answer")
            return "compose_answer"
        else:
            logger.warning(f"ü§ñ [RagAgent] POST - Unknown action '{next_action}', defaulting to compose_answer")
            return "compose_answer"


class FilterAgent(Node):
    """
    Filter candidates using LLM semantic understanding.

    Selects most relevant questions from candidates.
    Output: selected_ids (list of IDs)
    """

    def prep(self, shared):
        logger.info("üîç [FilterAgent] PREP - Reading query and candidates")
        query = shared.get("query", "")
        candidates = shared.get("retrieved_candidates", [])

        logger.info(f"üîç [FilterAgent] PREP - Query: '{query[:50]}...', Candidates: {len(candidates)}")
        return query, candidates

    def exec(self, inputs):
        from utils.llm import call_llm
        from utils.parsing import parse_yaml_with_schema
        from utils.auth import APIOverloadException
        from config.timeout_config import timeout_config

        query, candidates = inputs
        logger.info(f"üîç [FilterAgent] EXEC - Filtering {len(candidates)} candidates")

        # Handle empty candidates
        if not candidates:
            logger.warning("üîç [FilterAgent] EXEC - No candidates to filter")
            return []

        # Handle very few candidates (‚â§ 3) - return all
        if len(candidates) <= 3:
            logger.info(f"üîç [FilterAgent] EXEC - Only {len(candidates)} candidates, returning all")
            return [c["id"] for c in candidates]

        # Format candidates for LLM
        candidate_list_str = self._format_candidates(candidates)

        prompt = f"""Ch·ªçn t·ªëi ƒëa 6 c√¢u h·ªèi li√™n quan nh·∫•t ƒë·ªÉ tr·∫£ l·ªùi user.

User: "{query}"

Candidates:
{candidate_list_str}

YAML:
```yaml
selected_ids: [...]
```"""

        try:
            resp = call_llm(prompt, fast_mode=True, max_retry_time=timeout_config.LLM_RETRY_TIMEOUT)

            result = parse_yaml_with_schema(
                resp,
                required_fields=["selected_ids"],
                field_types={"selected_ids": list}
            )

            if result and result["selected_ids"]:
                # Cap at 6
                selected_ids = result["selected_ids"][:6]
                logger.info(f"üîç [FilterAgent] EXEC - Selected {len(selected_ids)} IDs")
                return selected_ids
            else:
                # Fallback: top 6
                logger.warning("üîç [FilterAgent] EXEC - LLM parsing failed, using top 6")
                return [c["id"] for c in candidates[:6]]

        except (APIOverloadException, Exception) as e:
            logger.warning(f"üîç [FilterAgent] EXEC - Error: {e}, using top 6")
            return [c["id"] for c in candidates[:6]]

    def _format_candidates(self, candidates: list) -> str:
        """Format candidates compactly for LLM prompt"""
        lines = []
        for i, c in enumerate(candidates, 1):
            question = c["CAUHOI"][:100] + "..." if len(c["CAUHOI"]) > 100 else c["CAUHOI"]
            lines.append(f"{i}. ID={c['id']}: \"{question}\"")
        return "\n".join(lines)

    def post(self, shared, prep_res, exec_res):
        # exec_res is just a list of IDs
        selected_ids = exec_res if isinstance(exec_res, list) else []

        # Save to shared store
        shared["selected_ids"] = selected_ids
        shared["rag_state"] = "filtered"

        logger.info(f"üîç [FilterAgent] POST - Saved {len(selected_ids)} IDs")

        return "default"


class RetrieveFromKB(Node):
    """
    Retrieve relevant QA pairs from Qdrant vector database using hybrid search.

    ID-based architecture - no scoring needed (FilterAgent handles semantic filtering):
    - prep(): Read query and metadata from shared
    - exec(): Call Qdrant retrieval utility
    - post(): Write lightweight {id, CAUHOI} to shared

    Output: shared["retrieved_candidates"] - list of lightweight candidates
    """

    def prep(self, shared):
        logger.info("üìö [RetrieveFromKB] PREP - ƒê·ªçc query v√† metadata t·ª´ shared")

        # Read from shared store ONLY
        query = shared.get("query", "")
        demuc = shared.get("demuc", "")
        chu_de_con = shared.get("chu_de_con", "")

        logger.info(f"üìö [RetrieveFromKB] PREP - query='{str(query)[:80]}...', demuc='{demuc}', chu_de_con='{chu_de_con}'")
        return query, demuc, chu_de_con

    def exec(self, inputs):
        query, demuc, chu_de_con = inputs
        logger.info("üìö [RetrieveFromKB] EXEC - B·∫Øt ƒë·∫ßu retrieve t·ª´ Qdrant")

        # Call Qdrant retrieval utility function
        from utils.knowledge_base.qdrant_retrieval import retrieve_from_qdrant

        # Retrieve with filters if available
        retrieved_results = retrieve_from_qdrant(
            query=query,
            demuc=demuc if demuc else None,
            chu_de_con=chu_de_con if chu_de_con else None,
            top_k=20
        )

        # Extract lightweight candidates: {id, CAUHOI}
        candidates = [
            {
                "id": result["id"],
                "CAUHOI": result["CAUHOI"]
            }
            for result in retrieved_results
        ]

        # Log top results
        if candidates:
            lines = ["\nüìö [RetrieveFromKB] TOP CANDIDATES:"]
            for i, candidate in enumerate(candidates[:5], 1):
                lines.append(
                    f"  {i}. id={candidate['id']} | Q: {candidate['CAUHOI'][:80]}..."
                )
            logger.info("\n".join(lines))

        logger.info(f"üìö [RetrieveFromKB] EXEC - Retrieved {len(candidates)} candidates")
        return candidates

    def post(self, shared, prep_res, exec_res):
        logger.info("üìö [RetrieveFromKB] POST - L∆∞u k·∫øt qu·∫£ retrieve")

        candidates = exec_res

        # Save lightweight candidates to shared store
        shared["retrieved_candidates"] = candidates

        # Update RAG state
        shared["rag_state"] = "retrieved"

        logger.info(f"üìö [RetrieveFromKB] POST - Saved {len(candidates)} candidates to 'retrieved_candidates'")

        return "default" 

class GreetingResponse(Node):
    """Deprecated: Ch√†o h·ªèi ƒë∆∞·ª£c gom v√†o ChitChatRespond."""
    def post(self, shared, prep_res, exec_res):
        return "default"

class ChitChatRespond(Node):
    """Node x·ª≠ l√Ω t·∫•t c·∫£ tr∆∞·ªùng h·ª£p kh√¥ng c·∫ßn RAG (bao g·ªìm ch√†o h·ªèi)."""

    def prep(self, shared):
        role = shared.get("role", "")
        query = shared.get("query", "")
        conversation_history = shared.get("conversation_history", [])
        return role, query, conversation_history

    def exec(self, inputs):
        # Import dependencies only when needed
        from utils.role_enum import PERSONA_BY_ROLE, ROLE_DESCRIPTION_BY_VALUE
        from utils.llm import call_llm, PROMPT_CHITCHAT_RESPONSE
        from utils.auth import APIOverloadException
        from config.timeout_config import timeout_config

        role, query, conversation_history = inputs
        # L·∫•y 3 c·∫∑p g·∫ßn nh·∫•t (6 tin)
        history_lines = []
        for msg in conversation_history[-6:]:
            try:
                who = msg.get("role")
                content = msg.get("content", "")
                history_lines.append(f"- {who}: {content}")
            except Exception:
                continue
        formatted_history = "\n".join(history_lines)

        # L·∫•y persona theo role (fallback an to√†n)
        if role in PERSONA_BY_ROLE:
            persona = PERSONA_BY_ROLE[role]
            audience = persona.get('audience', 'ng∆∞·ªùi d√πng ph·ªï th√¥ng')
            tone = persona.get('tone', 'th√¢n thi·ªán, r√µ r√†ng')
        else:
             audience, tone =  'ng∆∞·ªùi d√πng ph·ªï th√¥ng', 'th√¢n thi·ªán, r√µ r√†ng'

        # L·∫•y description t·ª´ role value
        role_purpose_description = ROLE_DESCRIPTION_BY_VALUE.get(role, "Ng∆∞·ªùi d√πng")

        prompt = PROMPT_CHITCHAT_RESPONSE.format(
            conversation_history=formatted_history,
            query=query,
            role=role,
            description=role_purpose_description,
            audience=audience,
            tone=tone
        )

        try:
            resp = call_llm(prompt, max_retry_time=timeout_config.LLM_RETRY_TIMEOUT)
        except APIOverloadException:
            # ƒê√°nh d·∫•u API overload ƒë·ªÉ route sang fallback
            resp = "C·∫£m ∆°n b·∫°n ƒë√£ chia s·∫ª. M√¨nh lu√¥n s·∫µn s√†ng h·ªó tr·ª£ v·ªÅ th√¥ng tin y khoa n·∫øu b·∫°n c·∫ßn nh√©!"
            return {"reply": resp, "api_overload": True}

        return {"reply": resp, "api_overload": False}

    def post(self, shared, prep_res, exec_res):
        shared["answer_obj"] = {"explain": exec_res.get("reply", ""), "preformatted": True}
        shared["explain"] = exec_res.get("reply", "")
        if exec_res.get("api_overload", False):
            return "fallback"
        return "default"



class ComposeAnswer(Node):
    def prep(self, shared):
        # Import dependencies
        from utils.knowledge_base.qdrant_retrieval import get_full_qa_by_ids

        role = shared.get("role", "")
        query = shared.get("query", "")
        selected_ids = shared.get("selected_ids", [])
        score = shared.get("retrieval_score", 0.0)
        conversation_history = shared.get("conversation_history", [])

        logger.info(f"‚úçÔ∏è [ComposeAnswer] PREP - Role: '{role}', Query: '{query[:50]}...', Selected IDs: {selected_ids}")

        # Fetch full QA data from Qdrant using IDs
        if selected_ids:
            retrieved_qa = get_full_qa_by_ids(selected_ids)
            logger.info(f"‚úçÔ∏è [ComposeAnswer] PREP - Retrieved {len(retrieved_qa)} full QA pairs from Qdrant")
        else:
            logger.warning("‚úçÔ∏è [ComposeAnswer] PREP - No selected IDs, using empty list")
            retrieved_qa = []

        return (role, query, retrieved_qa, score, conversation_history)

    def exec(self, inputs):
        # Import dependencies only when needed
        import time
        from utils.role_enum import PERSONA_BY_ROLE
        from utils.helpers import format_kb_qa_list, format_conversation_history
        from utils.llm import call_llm, PROMPT_COMPOSE_ANSWER
        from utils.parsing import parse_yaml_with_schema
        from utils.auth import APIOverloadException
        from config.timeout_config import timeout_config

        role, query, retrieved,  score, conversation_history = inputs

        # Handle missing or invalid role with fallback
        if role not in PERSONA_BY_ROLE:
            logger.warning(f"‚úçÔ∏è [ComposeAnswer] EXEC - Invalid role '{role}', using default patient_diabetes role")
            role = "patient_diabetes"  # Default fallback role

        persona = PERSONA_BY_ROLE[role]
        # Compact KB context
        relevant_info_from_kb = format_kb_qa_list(retrieved, max_items=6)

        # Format conversation history
        formatted_history = format_conversation_history(conversation_history)

        prompt = PROMPT_COMPOSE_ANSWER.format(
            ai_role=persona['persona'],
            audience=persona['audience'],
            tone=persona['tone'],
            query=query,
            relevant_info_from_kb=relevant_info_from_kb if relevant_info_from_kb else "Kh√¥ng c√≥ th√¥ng tin t·ª´ c∆° s·ªü tri th·ª©c",
            conversation_history = formatted_history
        )
        logger.info(f"‚úçÔ∏è [ComposeAnswer] EXEC - prompt: {prompt}")

        try:
            start_time = time.time()
            result = call_llm(prompt, max_retry_time=timeout_config.LLM_RETRY_TIMEOUT)
            end_time = time.time()

            # Log LLM timing

            logger.info(f"‚úçÔ∏è [ComposeAnswer] EXEC - LLM response received")
            result = parse_yaml_with_schema(result, required_fields=["explanation", "suggestion_questions"], field_types={"explanation": str, "suggestion_questions": list})
            logger.info(f"‚úçÔ∏è [ComposeAnswer] EXEC - result: {result}")

            if not result or  isinstance(result, str):
                logger.warning("[ComposeAnswer] EXEC - Invalid LLM response, using fallback")
                resp = "Xin l·ªói, t√¥i kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi ph√π h·ª£p l√∫c n√†y. B·∫°n ƒë·∫∑t c√¢u h·ªèi kh√°c ƒë∆∞·ª£c kh√¥ng? "
                return {"explain": resp, "suggestion_questions": [], "preformatted": True}

            return {"explain": result.get("explanation", ""), "suggestion_questions": result.get("suggestion_questions", []), "preformatted": True}

        except APIOverloadException as e:
            logger.warning(f"‚úçÔ∏è [ComposeAnswer] EXEC - API overloaded, triggering fallback mode: {e}")
            # Return flag to indicate API overload - will be handled in post method
            resp = "API hi·ªán ƒëang qu√° t·∫£i, ƒëang chuy·ªÉn sang ch·∫ø ƒë·ªô fallback..."
            return {"explain": resp, "suggestion_questions": [], "preformatted": True, "api_overload": True}


    def post(self, shared, prep_res, exec_res):
        logger.info("‚úçÔ∏è [ComposeAnswer] POST - L∆∞u answer object")
        shared["answer_obj"] = exec_res
        shared["explain"] = exec_res.get("explain", "")
        shared["suggestion_questions"] = exec_res.get("suggestion_questions", [])
        logger.info(f"‚úçÔ∏è [ComposeAnswer] POST - Answer keys: {list(exec_res.keys())}")
        logger.info(f"‚úçÔ∏è [ComposeAnswer] POST - Answer preview: {exec_res.get('explain')}")
        
        # Check if API overload occurred and route to fallback
        if exec_res.get("api_overload", False):
            logger.info("‚úçÔ∏è [ComposeAnswer] POST - API overloaded, routing to fallback")
            return "fallback"
        
        return "default"


class TopicClassifyAgent(Node):
    """
    Agent ph√¢n lo·∫°i ch·ªß ƒë·ªÅ ch√≠nh (DEMUC only).

    Refactored to follow PocketFlow best practices:
    - prep(): Read from shared store ONLY (no DB/API calls)
    - exec(): Call utility functions to classify DEMUC based on role's CSV file
    - post(): Write to shared store ONLY

    Classification:
    - Classify DEMUC from query based on role
    - CHU_DE_CON is always left empty (not classified)
    """

    def prep(self, shared):
        logger.info("üè∑Ô∏è [TopicClassifyAgent] PREP - ƒê·ªçc query v√† metadata t·ª´ shared")

        # Read ALL data from shared store - no external calls
        query = shared.get("query", "").strip()
        role = shared.get("role", "")
        current_demuc = shared.get("demuc", "")
        current_chu_de_con = shared.get("chu_de_con", "")

        logger.info(f"üè∑Ô∏è [TopicClassifyAgent] PREP - Role: '{role}', Query: '{query[:50]}...', DEMUC: '{current_demuc}'")

        return query, role, current_demuc, current_chu_de_con

    def exec(self, inputs):
        query, role, current_demuc, current_chu_de_con = inputs

        from utils.knowledge_base.metadata_utils import (
            get_demuc_list_for_role,
            format_demuc_list_for_prompt
        )
        from utils.llm.classify_topic import classify_demuc_with_llm

        # Only classify DEMUC (no CHU_DE_CON classification)
        logger.info(f"üè∑Ô∏è [TopicClassifyAgent] EXEC - Classifying DEMUC for query: '{query[:50]}...'")

        # Get DEMUC list for role
        demuc_list = get_demuc_list_for_role(role)
        if not demuc_list:
            logger.warning(f"üè∑Ô∏è [TopicClassifyAgent] EXEC - No DEMUC list found for role '{role}'")
            return {"demuc": "", "chu_de_con": "", "confidence": "low"}

        demuc_list_str = format_demuc_list_for_prompt(demuc_list)
        logger.info(f"üè∑Ô∏è [TopicClassifyAgent] EXEC - Available DEMUCs: {demuc_list}")

        # Classify DEMUC
        demuc_result = classify_demuc_with_llm(
            query=query,
            role=role,
            demuc_list_str=demuc_list_str
        )

        if demuc_result.get("api_overload"):
            return {"demuc": "", "chu_de_con": "", "confidence": "low", "api_overload": True}

        classified_demuc = demuc_result.get("demuc", "")
        logger.info(f"üè∑Ô∏è [TopicClassifyAgent] EXEC - Classification result: DEMUC='{classified_demuc}'")

        # Return with DEMUC only (no CHU_DE_CON)
        return {
            "demuc": classified_demuc,
            "chu_de_con": "",  # Always empty - we don't classify CHU_DE_CON
            "confidence": demuc_result.get("confidence", "low"),
            "reason": demuc_result.get("reason", "")
        }

    def post(self, shared, prep_res, exec_res):
        logger.info(f"üè∑Ô∏è [TopicClassifyAgent] POST - Classification result: {exec_res}")

        # Update shared with classification results - WRITE ONLY
        shared["demuc"] = exec_res.get("demuc", "")
        shared["chu_de_con"] = exec_res.get("chu_de_con", "")  # Always empty now
        shared["classification_confidence"] = exec_res.get("confidence", "low")

        logger.info(f"üè∑Ô∏è [TopicClassifyAgent] POST - Updated: DEMUC='{shared['demuc']}'")

        # Check for API overload
        if exec_res.get("api_overload", False):
            return "fallback"

        # Classification complete - proceed to retrieval
        logger.info("üè∑Ô∏è [TopicClassifyAgent] POST - Classification complete, routing to retrieval")
        return "default"  # Go to next node (RetrieveFromKB)


class QueryExpandAgent(Node):
    """Agent m·ªü r·ªông c√¢u h·ªèi m∆° h·ªì th√†nh c√¢u h·ªèi c·ª• th·ªÉ h∆°n"""

    def prep(self, shared):
        logger.info("üîç [QueryExpandAgent] PREP - ƒê·ªçc query v√† context")
        query = shared.get("query", "").strip()
        role = shared.get("role", "")
        conversation_history = shared.get("conversation_history", [])
        demuc = shared.get("demuc", "")
        chu_de_con = shared.get("chu_de_con", "")

        # Format conversation history
        history_lines = []
        for msg in conversation_history[-6:]:
            try:
                who = msg.get("role")
                content = msg.get("content", "")
                history_lines.append(f"- {who}: {content}")
            except Exception:
                continue
        formatted_history = "\n".join(history_lines)

        return query, role, demuc, chu_de_con, formatted_history

    def exec(self, inputs):
        # Import dependencies only when needed
        from utils.llm import call_llm
        from utils.parsing import parse_yaml_with_schema
        from utils.auth import APIOverloadException
        from config.timeout_config import timeout_config

        query, role, demuc, chu_de_con, formatted_history = inputs
        logger.info(f"üîç [QueryExpandAgent] EXEC - Query: '{query[:50]}...', DEMUC: '{demuc}', CHU_DE_CON: '{chu_de_con}'")

        # Build context about the topic classification
        topic_context = ""
        if demuc and chu_de_con:
            topic_context = f"\nƒê√£ x√°c ƒë·ªãnh ƒë∆∞·ª£c ch·ªß ƒë·ªÅ: DEMUC='{demuc}', CHU_DE_CON='{chu_de_con}'"

        prompt = f"""
B·∫°n l√† tr·ª£ l√Ω y khoa chuy√™n m·ªü r·ªông v√† l√†m r√µ c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.

Ng·ªØ c·∫£nh h·ªôi tho·∫°i g·∫ßn ƒë√¢y:
{formatted_history}

C√¢u h·ªèi hi·ªán t·∫°i c·ªßa ng∆∞·ªùi d√πng: "{query}"
Role c·ªßa ng∆∞·ªùi d√πng: {role}
{topic_context}

NHI·ªÜM V·ª§:
M·ªü r·ªông c√¢u h·ªèi th√†nh m·ªôt c√¢u h·ªèi C·ª§ TH·ªÇ H∆†N, R√ï R√ÄNG H∆†N, CHI TI·∫æT H∆†N.
- N·∫øu c√¢u h·ªèi ƒë√£ ƒë·ªß c·ª• th·ªÉ, c√≥ th·ªÉ gi·ªØ nguy√™n ho·∫∑c b·ªï sung chi ti·∫øt nh·ªè.
- N·∫øu c√¢u h·ªèi m∆° h·ªì, h√£y l√†m r√µ d·ª±a tr√™n ng·ªØ c·∫£nh h·ªôi tho·∫°i v√† ch·ªß ƒë·ªÅ ƒë√£ x√°c ƒë·ªãnh.

Y√äU C·∫¶U:
- expanded_query: c√¢u h·ªèi ƒë√£ ƒë∆∞·ª£c m·ªü r·ªông/c·ª• th·ªÉ h√≥a
- confidence: high/medium/low - m·ª©c ƒë·ªô t·ª± tin v·ªÅ vi·ªác m·ªü r·ªông ƒë√∫ng √Ω ng∆∞·ªùi d√πng
- reason: l√Ω do ng·∫Øn g·ªçn v·ªÅ c√°ch m·ªü r·ªông

V√ç D·ª§:
Input: "T√¥i mu·ªën h·ªèi v·ªÅ b·ªánh"
Context: DEMUC="B·ªÜNH L√ù ƒêTƒê", CHU_DE_CON="ƒê·ªãnh nghƒ©a v√† ph√¢n lo·∫°i"
Output:
```yaml
expanded_query: "ƒê·ªãnh nghƒ©a v√† ph√¢n lo·∫°i b·ªánh ƒë√°i th√°o ƒë∆∞·ªùng l√† g√¨?"
confidence: "high"
reason: "M·ªü r·ªông d·ª±a tr√™n ch·ªß ƒë·ªÅ ƒë√£ x√°c ƒë·ªãnh"
```

Tr·∫£ v·ªÅ CH·ªà m·ªôt code block YAML h·ª£p l·ªá:

```yaml
expanded_query: "C√¢u h·ªèi ƒë√£ m·ªü r·ªông"
confidence: "high"
reason: "L√Ω do ng·∫Øn g·ªçn"
```
"""

        try:
            resp = call_llm(prompt, fast_mode=True, max_retry_time=timeout_config.LLM_RETRY_TIMEOUT)
            logger.info(f"üîç [QueryExpandAgent] EXEC - LLM response: {resp}")

            result = parse_yaml_with_schema(
                resp,
                required_fields=["expanded_query"],
                optional_fields=["confidence", "reason"],
                field_types={"expanded_query": str, "confidence": str, "reason": str}
            )

            if result:
                logger.info(f"üîç [QueryExpandAgent] EXEC - Expanded result: {result}")
                return result
        except APIOverloadException as e:
            logger.warning(f"üîç [QueryExpandAgent] EXEC - API overloaded: {e}")
            return {"expanded_query": query, "confidence": "low", "api_overload": True}
        except Exception as e:
            logger.warning(f"üîç [QueryExpandAgent] EXEC - Expansion failed: {e}")

        # Fallback: return original query
        return {"expanded_query": query, "confidence": "low"}

    def post(self, shared, prep_res, exec_res):
        logger.info(f"üîç [QueryExpandAgent] POST - Expansion result: {exec_res}")

        # Update query with expanded version
        original_query = shared.get("query", "")
        expanded_query = exec_res.get("expanded_query", original_query)

        shared["original_query"] = original_query
        shared["query"] = expanded_query  # Replace with expanded query
        shared["expansion_confidence"] = exec_res.get("confidence", "low")

        logger.info(f"üîç [QueryExpandAgent] POST - Query expanded from '{original_query[:50]}...' to '{expanded_query[:50]}...'")

        # Check for API overload
        if exec_res.get("api_overload", False):
            return "fallback"

        # Update RAG state and route back to RagAgent
        shared["rag_state"] = "expanded"
        logger.info("üîç [QueryExpandAgent] POST - Routing back to RagAgent")
        return "default"


class MainDecisionAgent(Node):
    """Main decision agent - ONLY decides between RAG agent or chitchat agent"""

    def prep(self, shared):
        logger.info("[MainDecision] PREP - ƒê·ªçc query ƒë·ªÉ ph√¢n lo·∫°i RAG vs chitchat")
        query = shared.get("query", "").strip()
        role = shared.get("role", "")
        conversation_history = shared.get("conversation_history", [])
        # L·∫•y 3 c·∫∑p g·∫ßn nh·∫•t (6 tin)
        history_lines = []
        for msg in conversation_history[-6:]:
            try:
                who = msg.get("role")
                content = msg.get("content", "")
                history_lines.append(f"- {who}: {content}")
            except Exception:
                continue
        formatted_history = "\n".join(history_lines)
        return query, role, formatted_history

    def exec(self, inputs):
        # Import dependencies only when needed
        from utils.llm import call_llm
        from utils.parsing import parse_yaml_with_schema
        from utils.auth import APIOverloadException
        from config.timeout_config import timeout_config

        query, role, formatted_history = inputs
        logger.info("[MainDecision] EXEC - Deciding and responding")

        # Prompt: decide type AND generate response if direct_response
        prompt = f"""B·∫°n l√† tr·ª£ l√Ω y t·∫ø nha khoa v√† n·ªôi ti·∫øt. Ph√¢n t√≠ch c√¢u h·ªèi v√† quy·∫øt ƒë·ªãnh.

C√¢u h·ªèi: "{query}"

H√†nh ƒë·ªông:
- direct_response: trao ƒë·ªïi xu·ªìng s·∫£.  
- retrieve_kb: c√¢u h·ªèi v·ªÅ y t·∫ø c·∫ßn tra ki·∫øn th·ª©c y t·∫ø. 

Tr·∫£ v·ªÅ YAML:
```yaml
type: direct_response
explanation: "C√¢u tr·∫£ l·ªùi c·ªßa b·∫°n ·ªü ƒë√¢y"
```

HO·∫∂C n·∫øu c·∫ßn tra KB:
```yaml
type: retrieve_kb
explanation: ""
```"""

        try:
            resp = call_llm(prompt, fast_mode=True, max_retry_time=timeout_config.LLM_RETRY_TIMEOUT)

            result = parse_yaml_with_schema(
                resp,
                required_fields=["type"],
                optional_fields=["explanation"],
                field_types={"type": str, "explanation": str}
            )

            decision_type = result.get("type", "")
            explanation = result.get("explanation", "")

            logger.info(f"[MainDecision] EXEC - Type: {decision_type}, Explanation length: {len(explanation)}")

            return {"type": decision_type, "explanation": explanation}

        except APIOverloadException as e:
            logger.warning(f"[MainDecision] EXEC - API overloaded, triggering fallback: {e}")
            return {"type": "api_overload", "explanation": ""}
        except Exception as e:
            logger.warning(f"[MainDecision] EXEC - LLM classification failed: {e}")
            return {"type": "default", "explanation": ""}

    def post(self, shared, prep_res, exec_res):
        logger.info(f"[MainDecision] POST - Classification result: {exec_res}")
        input_type = exec_res.get("type", "")
        explanation = exec_res.get("explanation", "")

        # Save explanation to shared if direct_response
        if input_type == "direct_response" and explanation:
            shared["answer_obj"] = {
                "explain": explanation,
                "preformatted": True,
                "suggestion_questions": []
            }
            shared["explain"] = explanation
            shared["suggestion_questions"] = []
            logger.info(f"[MainDecision] POST - Direct response saved to 'explain': {explanation[:80]}...")
            return "direct_response"
        elif input_type == "retrieve_kb":
            # Initialize retrieve attempts counter for RAG pipeline
            shared["retrieve_attempts"] = 0
            logger.info("[MainDecision] POST - Complex question, routing to retrieve_kb (attempts=0)")
            return "retrieve_kb"
        elif input_type == "api_overload" or input_type == "default":
            logger.warning("[MainDecision] POST - API issue, routing to fallback")
            return "fallback"
        else:
            # Fallback: if unknown type or no explanation, route to fallback
            logger.warning(f"[MainDecision] POST - Unknown type '{input_type}', routing to fallback")
            return "fallback"

class FallbackNode(Node):
    """Node fallback khi API qu√° t·∫£i - retrieve query v√† tr·∫£ k·∫øt qu·∫£ d·ª±a tr√™n score"""
    
    def prep(self, shared):
        logger.info("üîÑ [FallbackNode] PREP - X·ª≠ l√Ω fallback khi API qu√° t·∫£i")
        query = shared.get("query", "")
        role = shared.get("role", "")
        return query, role

    def exec(self, inputs):
        # Import dependencies only when needed
        from unidecode import unidecode
        from utils.knowledge_base import get_kb, ROLE_TO_CSV, retrieve_random_by_role
        from utils.helpers import aggregate_retrievals, format_kb_qa_list

        query, role = inputs
        logger.info(f"üîÑ [FallbackNode] EXEC - Fallback search cho role: {role} v·ªõi query: '{query[:50]}...'")

        try:
            # 1) T√¨m tu·∫ßn t·ª± trong CSV theo role, so kh·ªõp HO√ÄN TO√ÄN v·ªõi c·ªôt CAUHOI
            kb = get_kb()
            role_lower = (role or "").lower()
            role_csv = ROLE_TO_CSV.get(role_lower)

            def _norm_text(s: str) -> str:
                s = unidecode((s or "").lower())
                return " ".join(s.split())

            q_norm = _norm_text(query)
            exact_matches = []

            if role_csv and role_csv in kb.role_dataframes:
                df = kb.role_dataframes[role_csv]
                for _, row in df.iterrows():
                    q_text = str(row.get("CAUHOI", ""))
                    a_text = str(row.get("CAUTRALOI", ""))
                    qn = _norm_text(q_text)
                    if qn and q_norm and qn == q_norm:
                        exact_matches.append({
                            "cau_hoi": q_text,
                            "cau_tra_loi": a_text,
                            "de_muc": row.get("DEMUC", ""),
                            "chu_de_con": row.get("CHUDECON", ""),
                            "ma_so": row.get("MASO", ""),
                            "keywords": row.get("keywords", ""),
                            "giai_thich": row.get("GIAITHICH", ""),
                        })

            # Build retrieval queries: use only the main query
            retrieval_queries = []
            if query:
                retrieval_queries.append(query)

            # Use aggregate_retrievals helper function
            retrieved_results, _ = aggregate_retrievals(retrieval_queries, role=role, top_k=15)

            try:
                formatted = format_kb_qa_list(retrieved_results, max_items=15)
                if formatted:
                    logger.info("\nüìö [FallbackNode] RETRIEVE - Aggregated Results:\n" + formatted)
            except Exception:
                pass

            # Log th√™m b·∫£ng ƒëi·ªÉm cho retrieved_results
            if retrieved_results:
                lines = ["\nüè∑Ô∏è [FallbackNode] TOP SCORES (desc):"]
                for i, it in enumerate(retrieved_results, 1):
                    q = str(it.get('cau_hoi', ''))
                    sc = float(it.get('score', 0.0))
                    lines.append(f"  {i}. score={sc:.4f} | Q: {q[:140]}")
                logger.info("\n".join(lines))

            if exact_matches:
                best = exact_matches[0]
                explain = best.get("cau_tra_loi", "")
                # Suggestions: top4 t·ª´ retrieve (kh√°c c√¢u exact match)
                suggestion_questions = []
                exact_q_norm = _norm_text(best.get("cau_hoi", ""))
                for it in retrieved_results:
                    q = it.get('cau_hoi', '')
                    if q and _norm_text(q) != exact_q_norm:
                        suggestion_questions.append(q)
                        if len(suggestion_questions) >= 4:
                            break
                score = 1.0
                # Log l·ª±a ch·ªçn cu·ªëi
                logger.info("\n‚úÖ [FallbackNode] EXPLAIN (exact match): score=1.0000 | Q (exact): " + str(best.get("cau_hoi", ""))[:140])
                if suggestion_questions:
                    # map score theo c√¢u h·ªèi ƒë·ªÉ log
                    score_map = {str(it.get('cau_hoi', '')): float(it.get('score', 0.0)) for it in retrieved_results}
                    sug_lines = ["üìå [FallbackNode] SUGGESTIONS (top4):"]
                    for idx, sq in enumerate(suggestion_questions, 1):
                        sug_lines.append(f"  {idx}. score={score_map.get(sq, 0.0):.4f} | Q: {sq[:140]}")
                    logger.info("\n".join(sug_lines))
            else:
                # Kh√¥ng c√≥ exact match: n·∫øu c√≥ retrieved_results, d√πng top1 l√†m explain v√† c√≤n l·∫°i l√†m suggestion
                if retrieved_results:
                    best_answer = retrieved_results[0]
                    explain = best_answer.get("cau_tra_loi", "")
                    suggestion_questions = [it.get('cau_hoi', '') for it in retrieved_results[1:5] if it.get('cau_hoi')]
                    score = float(best_answer.get('score', 0.0))
                    # Log l·ª±a ch·ªçn cu·ªëi
                    logger.info(f"\n‚úÖ [FallbackNode] EXPLAIN (retrieve top1): score={score:.4f} | Q: {str(best_answer.get('cau_hoi',''))[:140]}")
                    if suggestion_questions:
                        sug_lines = ["üìå [FallbackNode] SUGGESTIONS (next4):"]
                        for idx, it in enumerate(retrieved_results[1:5], 1):
                            if not it.get('cau_hoi'):
                                continue
                            sug_lines.append(f"  {idx}. score={float(it.get('score', 0.0)):.4f} | Q: {str(it.get('cau_hoi'))[:140]}")
                        logger.info("\n".join(sug_lines))
                else:
                    explain = "Hi·ªán t·∫°i t√¥i ch∆∞a c√≥ ƒë·ªß th√¥ng tin li√™n quan ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y c·ªßa b·∫°n, B·∫°n c√≥ th·ªÉ ƒë·∫∑t l·∫°i c√¢u h·ªèi kh√°c ho·∫∑c di·ªÖn ƒë·∫°t l·∫°i c√¢u h·ªèi c·ªßa b·∫°n! Ho·∫∑c b·∫°n c√≥ th·ªÉ ch·ªçn c√°c c√¢u h·ªèi g·ª£i √Ω d∆∞·ªõi ƒë√¢y!"
                    random_questions = retrieve_random_by_role(role, amount=5)
                    suggestion_questions = [q['cau_hoi'] for q in random_questions]
                    score = 0.0

            result = {
                "explain": explain,
                "suggestion_questions": suggestion_questions,
                "retrieval_score": score,
                "preformatted": True
            }

            logger.info(f"üîÑ [FallbackNode] EXEC - Generated response with {len(suggestion_questions)} suggestions")
            return result

        except Exception as e:
            logger.error(f"üîÑ [FallbackNode] EXEC - Error during fallback: {e}")
            # Fallback t·ªëi thi·ªÉu
            return {
                "explain": "Xin l·ªói, h·ªá th·ªëng ƒëang g·∫∑p s·ª± c·ªë. Vui l√≤ng th·ª≠ l·∫°i sau.",
                "suggestion_questions": [],
                "retrieval_score": 0.0,
                "preformatted": True
            }
    
    def post(self, shared, prep_res, exec_res):
        logger.info("üîÑ [FallbackNode] POST - L∆∞u fallback response")
        shared["answer_obj"] = exec_res
        shared["explain"] = exec_res.get("explain", "")
        shared["suggestion_questions"] = exec_res.get("suggestion_questions", [])
        shared["retrieval_score"] = exec_res.get("retrieval_score", 0.0)
        return "default"

