from math import log
from unittest import result
from pocketflow import Node
from utils.call_llm import call_llm, APIOverloadException
from utils.kb import retrieve, retrieve_random_by_role

from utils.response_parser import parse_yaml_response, validate_yaml_structure, parse_yaml_with_schema
from utils.prompts import (
    PROMPT_CLASSIFY_INPUT, 
    PROMPT_COMPOSE_ANSWER
)
from utils.helpers import (
    format_kb_qa_list,
    get_score_threshold,
    format_conversation_history,
    log_llm_timing
)
from utils.role_ENUM import (
    PERSONA_BY_ROLE
)
from typing import Any, Dict, List, Tuple
import textwrap
import yaml
import logging
import re
import time

# Configure logging for this module
logger = logging.getLogger(__name__)

class AnswerNode(Node):
    def prep(self, shared):
        return shared["question"]
    
    def exec(self, question):
        start_time = time.time()
        result = call_llm(question)
        end_time = time.time()
        
        # Log LLM timing
        log_llm_timing("AnswerNode", start_time, end_time, len(question))
        
        return result
    
    def post(self, shared, prep_res, exec_res):
        shared["answer"] = exec_res




# ========== Medical Agent Nodes ==========



class IngestQuery(Node):
    def prep(self, shared):
        logger.info("ğŸ” [IngestQuery] PREP - Äá»c role vÃ  input tá»« shared")
        role = shared.get("role", "")
        user_input = shared.get("input", "")
        logger.info(f"ğŸ” [IngestQuery] PREP - Role: {role}, Users Input : {user_input}")
        return role, user_input

    def exec(self, inputs):
        logger.info("ğŸ” [IngestQuery] EXEC - Xá»­ lÃ½ role vÃ  query")
        role, user_input = inputs
        result = {"role": role, "query": user_input.strip()}
        logger.info(f"ğŸ” [IngestQuery] EXEC - Processed: {result}")
        return result

    def post(self, shared, prep_res, exec_res):
        logger.info("ğŸ” [IngestQuery] POST - LÆ°u role vÃ  query vÃ o shared")
        shared["role"] = exec_res["role"]
        shared["query"] = exec_res["query"]
        logger.info(f"ğŸ” [IngestQuery] POST - Saved role: {exec_res['role']}, query: {exec_res['query'][:50]}...")
        return "default"

class RetrieveFromKB(Node):
    def prep(self, shared):
        logger.info("ğŸ“š [RetrieveFromKB] PREP - Äá»c query vÃ  rag_questions Ä‘á»ƒ retrieve")
        query = shared.get("query", "")
        rag_questions = shared.get("rag_questions", [])
        # Káº¿t há»£p query gá»‘c vá»›i cÃ¡c cÃ¢u há»i RAG Ä‘á»ƒ tÃ¬m kiáº¿m toÃ n diá»‡n hÆ¡n
        all_queries = [query] + rag_questions
        search_term = " ".join(all_queries)   
        user_role =  shared.get("role", "")
        logger.info(f"ğŸ“š [RetrieveFromKB] PREP - Search Term: '{search_term[:100]}...'")
        return search_term, user_role

    def exec(self, inputs):
        search_term, user_role = inputs
        logger.info("ğŸ“š [RetrieveFromKB] EXEC - Báº¯t Ä‘áº§u retrieve tá»« knowledge base")
        logger.info(f"ğŸ“š [RetrieveFromKB] EXEC - Query: {search_term}")
        import time

        start_time = time.time()
        # Reduce retrieval breadth
        results, score = retrieve(search_term, user_role, top_k=5)
        elapsed_time = time.time() - start_time

        # Log elapsed time to a file
        with open("retrieve_timing.log", "a", encoding="utf-8") as f:
            f.write(f" Time: {elapsed_time:.4f} seconds\n")
        logger.info(f"ğŸ“š [RetrieveFromKB] EXEC - Retrieved results: {results} , best score: {score:.4f}")
        return results, score

    def post(self, shared, prep_res, exec_res):
        logger.info("ğŸ“š [RetrieveFromKB] POST - LÆ°u káº¿t quáº£ retrieve")
        results, score = exec_res
        shared["retrieved"] = results
        shared["retrieval_score"] = score
        shared["need_clarify"] = score < get_score_threshold()
        
        # Always continue to next node via default edge (ScoreDecisionNode)
        input_type = shared.get("input_type", "medical_question")
        logger.info(
            f"ğŸ“š [RetrieveFromKB] POST - Saved {len(results)} results, score: {score:.4f}, "
            f"input_type={input_type} -> routing via 'default' to ScoreDecision"
        )
        return "default" 

class GreetingResponse(Node):
    """Node xá»­ lÃ½ chÃ o há»i - set context vÃ  route Ä‘áº¿n topic suggestion"""
    def prep(self, shared):
        return shared.get("role", ""), shared.get("query", "")
    
    def exec(self, inputs):
        role, query = inputs
        return {"context_set": True, "role": role, "query": query}
    
    def post(self, shared, prep_res, exec_res):
        shared["explain"] = "Xin chÃ o ğŸ˜Š! TÃ´i lÃ  trá»£ lÃ½ AI cá»§a báº¡n. Ráº¥t vui Ä‘Æ°á»£c há»— trá»£ báº¡n - Báº¡n cáº§n tÃ´i giÃºp gÃ¬ hÃ´m nay? "
        return "default"


class ComposeAnswer(Node):
    def prep(self, shared):
        role = shared.get("role", "")
        query = shared.get("query", "")
        retrieved = shared.get("retrieved", [])
        score = shared.get("retrieval_score", 0.0)
        conversation_history = shared.get("conversation_history", [])
        logger.info(f"âœï¸ [ComposeAnswer] PREP - Role: '{role}', Query: '{query[:50]}...', Retrieved: {len(retrieved)} items")
        return (role, query, retrieved, score, conversation_history)

    def exec(self, inputs):
        role, query, retrieved,  score, conversation_history = inputs
        
        # Handle missing or invalid role with fallback
        if role not in PERSONA_BY_ROLE:
            logger.warning(f"âœï¸ [ComposeAnswer] EXEC - Invalid role '{role}', using default patient_diabetes role")
            role = "patient_diabetes"  # Default fallback role
        
        persona = PERSONA_BY_ROLE[role]
        # Compact KB context
        relevant_info_from_kb = format_kb_qa_list(retrieved, max_items=6)
        
        # Format conversation history
        formatted_history = format_conversation_history(conversation_history)
        
        prompt = PROMPT_COMPOSE_ANSWER.format(
            ai_role=persona['persona'],
            audience=persona['audience'],
            tone=persona['tone'],
            query=query,
            relevant_info_from_kb=relevant_info_from_kb if relevant_info_from_kb else "KhÃ´ng cÃ³ thÃ´ng tin tá»« cÆ¡ sá»Ÿ tri thá»©c",
            conversation_history = formatted_history
        )
        logger.info(f"âœï¸ [ComposeAnswer] EXEC - prompt: {prompt}")
        
        try:
            start_time = time.time()
            result = call_llm(prompt)
            end_time = time.time()
            
            # Log LLM timing
            log_llm_timing("ComposeAnswer", start_time, end_time, len(prompt))
            
            logger.info(f"âœï¸ [ComposeAnswer] EXEC - LLM response received")
            result = parse_yaml_with_schema(result, required_fields=["explanation", "suggestion_questions"], field_types={"explanation": str, "suggestion_questions": list})
            logger.info(f"âœï¸ [ComposeAnswer] EXEC - result: {result}")

            if not result or  isinstance(result, str):
                logger.warning("[ComposeAnswer] EXEC - Invalid LLM response, using fallback")
                resp = "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ táº¡o cÃ¢u tráº£ lá»i phÃ¹ há»£p lÃºc nÃ y. Báº¡n Ä‘áº·t cÃ¢u há»i khÃ¡c Ä‘Æ°á»£c khÃ´ng? "
                return {"explain": resp, "suggestion_questions": [], "preformatted": True}
            
            return {"explain": result.get("explanation", ""), "suggestion_questions": result.get("suggestion_questions", []), "preformatted": True}
        
        except APIOverloadException as e:
            logger.warning(f"âœï¸ [ComposeAnswer] EXEC - API overloaded, triggering fallback mode: {e}")
            # Return flag to indicate API overload - will be handled in post method
            resp = "API hiá»‡n Ä‘ang quÃ¡ táº£i, Ä‘ang chuyá»ƒn sang cháº¿ Ä‘á»™ fallback..."
            return {"explain": resp, "suggestion_questions": [], "preformatted": True, "api_overload": True}


    def post(self, shared, prep_res, exec_res):
        logger.info("âœï¸ [ComposeAnswer] POST - LÆ°u answer object")
        shared["answer_obj"] = exec_res
        shared["explain"] = exec_res.get("explain", "")
        shared["suggestion_questions"] = exec_res.get("suggestion_questions", [])
        logger.info(f"âœï¸ [ComposeAnswer] POST - Answer keys: {list(exec_res.keys())}")
        logger.info(f"âœï¸ [ComposeAnswer] POST - Answer preview: {exec_res.get('explain')}")
        
        # Check if API overload occurred and route to fallback
        if exec_res.get("api_overload", False):
            logger.info("âœï¸ [ComposeAnswer] POST - API overloaded, routing to fallback")
            return "fallback"
        
        return "default"



class ClarifyQuestionNode(Node):
    """Node xá»­ lÃ½ clarification cho medical questions cÃ³ score tháº¥p"""
    
    def prep(self, shared):
        role = shared.get("role", "")
        query = shared.get("query", "")
        retrieved = shared.get("retrieved", [])
        rag_questions = shared.get("rag_questions", [])
        logger.info(f"[ClarifyQuestion] PREP - Role: {role}, Query: '{query[:50]}...', RAG Questions: {len(rag_questions)}")
        return role, query, retrieved, rag_questions
    
    def exec(self, inputs):
        role, query, retrieved, rag_questions = inputs
        logger.info(f"[ClarifyQuestion] EXEC - Generating clarification for low-score medical query")
        
        # Láº¥y danh sÃ¡ch cÃ¢u há»i tá»« retrieved hoáº·c random náº¿u retrieved trá»‘ng
        if not retrieved:
            suggestion_questions = [q['cau_hoi'] for q in retrieve_random_by_role(role, amount=4)]
        else:
            # Láº¥y cÃ¢u há»i tá»« retrieved data
            suggestion_questions = [item.get('cau_hoi', '') for item in retrieved if item.get('cau_hoi')][:5]
        
        
        result = {
            "explain": "Hiá»‡n táº¡i mÃ¬nh chÆ°a cÃ³ Ä‘á»§ thÃ´ng tin liÃªn quan Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i nÃ y cá»§a báº¡n, Báº¡n cÃ³ thá»ƒ Ä‘áº·t láº¡i cÃ¢u há»i khÃ¡c hoáº·c diá»…n Ä‘áº¡t láº¡i cÃ¢u há»i cá»§a báº¡n! Hoáº·c báº¡n cÃ³ thá»ƒ chá»n cÃ¡c cÃ¢u há»i gá»£i Ã½ dÆ°á»›i Ä‘Ã¢y!",
            "suggestion_questions": suggestion_questions,
            "preformatted": True,
        }
        
        logger.info(f"[ClarifyQuestion] EXEC - Generated {len(suggestion_questions)} clarification questions")
        return result
    
    def post(self, shared, prep_res, exec_res):
        logger.info("[ClarifyQuestion] POST - LÆ°u clarification response")
        shared["answer_obj"] = exec_res
        shared["explain"] = exec_res.get("explain", "")
        shared["suggestion_questions"] = exec_res.get("suggestion_questions", [])
        return "default"


class TopicSuggestResponse(Node):
    """Node xá»­ lÃ½ gá»£i Ã½ topic khi user yÃªu cáº§u gá»£i Ã½ chá»§ Ä‘á»"""
    def prep(self, shared):
        role = shared.get("role", "")
        query = shared.get("query", "")
        logger.info(f"[TopicSuggestResponse] PREP - Role: {role}, Query: '{query[:50]}...'")
        return role, query
    
    def exec(self, inputs):
        role, query = inputs
        logger.info(f"[TopicSuggestResponse] EXEC - Generating topic suggestions for role: {role}")
        
        # Get fewer topic suggestions to reduce tokens
        suggestion_questions = [q['cau_hoi'] for q in retrieve_random_by_role(role, amount=5)]
        
        result = {
            "explain": "MÃ¬nh gá»£i Ã½ báº¡n cÃ¡c chá»§ Ä‘á» sau nhÃ©! Báº¡n cÃ³ thá»ƒ chá»n báº¥t ká»³ chá»§ Ä‘á» nÃ o mÃ  báº¡n quan tÃ¢m ğŸ˜Š",
            "suggestion_questions": suggestion_questions,
            "preformatted": True,
        }
        
        logger.info(f"[TopicSuggestResponse] EXEC - Generated {len(suggestion_questions)} topic suggestions")
        return result
    
    def post(self, shared, prep_res, exec_res):
        logger.info("[TopicSuggestResponse] POST - LÆ°u topic suggestion response")
        shared["answer_obj"] = exec_res
        shared["explain"] = exec_res.get("explain", "")
        shared["suggestion_questions"] = exec_res.get("suggestion_questions", [])
        return "default"




class MainDecisionAgent(Node):
    """Main decision agent - chá»‰ phÃ¢n loáº¡i input vÃ  routing"""
    
    def prep(self, shared):
        logger.info("[MainDecision] PREP - Äá»c query Ä‘á»ƒ phÃ¢n loáº¡i")
        query = shared.get("query", "").strip()
        role = shared.get("role", "")
        return query, role
    
    def exec(self, inputs):
        query, role = inputs
        logger.info("[MainDecision] EXEC - Using LLM for classification")
        prompt = PROMPT_CLASSIFY_INPUT.format(query=query, role=role)
        
        try:
            start_time = time.time()
            resp = call_llm(prompt)
            end_time = time.time()
            
            # Log LLM timing
            log_llm_timing("MainDecisionAgent", start_time, end_time, len(prompt))
            
            logger.info(f"[MainDecision] EXEC - resp: {resp}")
            result = parse_yaml_with_schema(
                resp,
                required_fields=["type"],
                optional_fields=["confidence", "reason", "rag_questions"],
                field_types={"type": str, "confidence": str, "reason": str, "rag_questions": list}
            )
            logger.info(f"[MainDecision] EXEC - result after parse: {result}")
            
            if result:
                logger.info(f"[MainDecision] EXEC - LLM classification: {result}")
                return result       
        except APIOverloadException as e:
            logger.warning(f"[MainDecision] EXEC - API overloaded, triggering fallback: {e}")
            return {"type": "api_overload", "confidence": "high", "rag_questions": []}
        except Exception as e:
            logger.warning(f"[MainDecision] EXEC - LLM classification failed: {e}")
        
        return {"type": "default", "confidence": "high", "rag_questions": []}
    
    def post(self, shared, prep_res, exec_res):
        logger.info(f"[MainDecision] POST - Classification result: {exec_res}")
        shared["input_type"] = exec_res["type"]
        shared["classification_confidence"] = exec_res.get("confidence", "low")
        shared["classification_reason"] = exec_res.get("reason", "")
        shared["rag_questions"] = exec_res.get("rag_questions", [])
        
        # Route based on classification
        input_type = exec_res["type"]
        
        if input_type == "medical_question":
            return "retrieve_kb"
        elif input_type == "greeting":
            return "greeting"
        elif input_type == "api_overload" or input_type == "default":
            return "fallback"
        else:
            return "topic_suggest"


class FallbackNode(Node):
    """Node fallback khi API quÃ¡ táº£i - retrieve query vÃ  tráº£ káº¿t quáº£ dá»±a trÃªn score"""
    
    def prep(self, shared):
        logger.info("ğŸ”„ [FallbackNode] PREP - Xá»­ lÃ½ fallback khi API quÃ¡ táº£i")
        query = shared.get("query", "")
        role = shared.get("role", "")
        return query, role
    
    def exec(self, inputs):
        query, role = inputs
        logger.info(f"ğŸ”„ [FallbackNode] EXEC - Retrieve tá»« query: '{query[:50]}...' cho role: {role}")
        
        try:
            # Retrieve tá»« knowledge base
            results, score = retrieve(query, role, top_k=5)
            logger.info(f"ğŸ”„ [FallbackNode] EXEC - Retrieved {len(results)} results, best score: {score:.4f}")
            logger.info(f"ğŸ”„ [FallbackNode] EXEC - Results: {results}")
            # Kiá»ƒm tra score threshold
            if score > 0.35:
                # CÃ³ káº¿t quáº£ tá»‘t - láº¥y cÃ¢u tráº£ lá»i cÃ³ score cao nháº¥t
                best_answer = results[0] if results else None
                if best_answer:
                    explain = best_answer.get("cau_tra_loi", "")
                    # Láº¥y thÃªm cÃ¢u há»i gá»£i Ã½ tá»« káº¿t quáº£ retrieve
                    suggestion_questions = [item.get('cau_hoi', '') for item in results[1:4] if item.get('cau_hoi')]
                else:
                    explain = "Xin lá»—i, khÃ´ng thá»ƒ láº¥y Ä‘Æ°á»£c thÃ´ng tin phÃ¹ há»£p lÃºc nÃ y."
                    suggestion_questions = []
            else:
                # Score tháº¥p - tráº£ vá» thÃ´ng bÃ¡o máº·c Ä‘á»‹nh + cÃ¢u há»i gá»£i Ã½ tá»« retrieve
                explain = "Hiá»‡n táº¡i mÃ¬nh chÆ°a cÃ³ Ä‘á»§ thÃ´ng tin liÃªn quan Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i nÃ y cá»§a báº¡n, Báº¡n cÃ³ thá»ƒ Ä‘áº·t láº¡i cÃ¢u há»i khÃ¡c hoáº·c diá»…n Ä‘áº¡t láº¡i cÃ¢u há»i cá»§a báº¡n! Hoáº·c báº¡n cÃ³ thá»ƒ chá»n cÃ¡c cÃ¢u há»i gá»£i Ã½ dÆ°á»›i Ä‘Ã¢y!"
                # Láº¥y cÃ¢u há»i gá»£i Ã½ tá»« káº¿t quáº£ retrieve (náº¿u cÃ³), fallback sang random náº¿u khÃ´ng cÃ³
                if results and len(results) > 0:
                    suggestion_questions = [item.get('cau_hoi', '') for item in results if item.get('cau_hoi')][:5]
                    # Náº¿u khÃ´ng Ä‘á»§ cÃ¢u há»i tá»« retrieve, bá»• sung thÃªm tá»« random
                    if len(suggestion_questions) < 3:
                        random_questions = retrieve_random_by_role(role, amount=5-len(suggestion_questions))
                        suggestion_questions.extend([q['cau_hoi'] for q in random_questions])
                else:
                    # KhÃ´ng cÃ³ káº¿t quáº£ retrieve, dÃ¹ng random
                    random_questions = retrieve_random_by_role(role, amount=5)
                    suggestion_questions = [q['cau_hoi'] for q in random_questions]
            
            result = {
                "explain": explain,
                "suggestion_questions": suggestion_questions,
                "retrieval_score": score,
                "preformatted": True
            }
            
            logger.info(f"ğŸ”„ [FallbackNode] EXEC - Generated response with {len(suggestion_questions)} suggestions")
            return result
            
        except Exception as e:
            logger.error(f"ğŸ”„ [FallbackNode] EXEC - Error during fallback: {e}")
            # Fallback tá»‘i thiá»ƒu
            return {
                "explain": "Xin lá»—i, há»‡ thá»‘ng Ä‘ang gáº·p sá»± cá»‘. Vui lÃ²ng thá»­ láº¡i sau.",
                "suggestion_questions": [],
                "retrieval_score": 0.0,
                "preformatted": True
            }
    
    def post(self, shared, prep_res, exec_res):
        logger.info("ğŸ”„ [FallbackNode] POST - LÆ°u fallback response")
        shared["answer_obj"] = exec_res
        shared["explain"] = exec_res.get("explain", "")
        shared["suggestion_questions"] = exec_res.get("suggestion_questions", [])
        shared["retrieval_score"] = exec_res.get("retrieval_score", 0.0)
        return "default"


class ScoreDecisionNode(Node):
    """Node quyáº¿t Ä‘á»‹nh dá»±a trÃªn retrieval score"""
    
    def prep(self, shared):
        logger.info("[ScoreDecision] PREP - Kiá»ƒm tra retrieval score")
        input_type = shared.get("input_type", "")
        retrieval_score = shared.get("retrieval_score", 0.0)
        return input_type, retrieval_score
    
    
    def exec(self, inputs):
        input_type, retrieval_score = inputs
        score_threshold = get_score_threshold()
        
        logger.info(f"[ScoreDecision] EXEC - Input: '{input_type}', Score: {retrieval_score:.4f}, Threshold: {score_threshold}")
        
        if input_type == "medical_question":

            if retrieval_score >= score_threshold:
                return {"action": "compose_answer", "context": "medical_high_score"}
            else:
                return {"action": "clarify", "context": "medical_low_score"}
            
        return {"action": "clarify", "context": "topic_suggestion"}
   
    def post(self, shared, prep_res, exec_res):
        shared["response_context"] = exec_res["context"]
        logger.info(f"[ScoreDecision] POST - Decision: {exec_res['action']}, Context: {exec_res['context']}")
        return exec_res["action"]
