from math import log
from unittest import result
from pocketflow import Node
from utils.call_llm import call_llm
from utils.kb import retrieve, retrieve_random_by_role
from utils.conversation_logger import log_user_message, log_bot_response, log_conversation_exchange
from utils.response_parser import parse_yaml_response, validate_yaml_structure, parse_yaml_with_schema
from utils.prompts import (
    PROMPT_CLASSIFY_INPUT, 
    PROMPT_CLARIFYING_QUESTIONS_GENERIC,
    PROMPT_COMPOSE_ANSWER,
    PROMPT_SUGGEST_FOLLOWUPS
)
from utils.helpers import (
    get_persona_for,
    get_topics_by_role,
    get_fallback_topics_by_role,
    get_most_relevant_QA,
    get_context_for_input_type,
    get_context_for_knowledge_case,
    get_score_threshold,
    
)
from typing import Any, Dict, List, Tuple
import textwrap
import yaml
import logging
import re

# Configure logging for this module
logger = logging.getLogger(__name__)


class AnswerNode(Node):
    def prep(self, shared):
        # Read question from shared
        return shared["question"]
    
    def exec(self, question):
        # Call LLM to get the answer
        return call_llm(question)
    
    def post(self, shared, prep_res, exec_res):
        # Store the answer in shared
        shared["answer"] = exec_res



# Removed _persona_for function - now imported from utils.helpers

# ========== Medical Agent Nodes ==========

class ClassifyInput(Node):
    """Node Ä‘á»ƒ phÃ¢n loáº¡i input cá»§a user thÃ nh cÃ¡c loáº¡i khÃ¡c nhau"""
    
    def prep(self, shared):
        logger.info("[ClassifyInput] PREP - Äá»c query Ä‘á»ƒ phÃ¢n loáº¡i")
        query = shared.get("query", "").strip()
        role = shared.get("role", "")
        logger.info(f"[ClassifyInput] PREP - Query: '{query}', Role: {role}")
        return query, role
    
    def exec(self, inputs):
        query, role = inputs

        pattern_result = classify_input_pattern(query)
        if pattern_result["confidence"] in ["high", "medium"]:
            logger.info(f"[ClassifyInput] EXEC - Pattern classification: {pattern_result}")
            return pattern_result
        
        # For ambiguous cases, use LLM
        if len(query) > 3:
            logger.info("[ClassifyInput] EXEC - Using LLM for classification")
            prompt = PROMPT_CLASSIFY_INPUT.format(query=query, role=role)
            
            try:
                resp = call_llm(prompt)
                result = parse_yaml_with_schema(
                    resp,
                    required_fields=["type"],
                    optional_fields=["confidence", "reason"],
                    field_types={"type": str, "confidence": str, "reason": str}
                )
                logger.info(f"[ClassifyInput] EXEC -resp {resp}")
                logger.info(f"[ClassifyInput] EXEC - result {result}")

                if result:
                    logger.info(f"[ClassifyInput] EXEC - LLM classification: {result}")
                    return result
            except Exception as e:
                logger.warning(f"[ClassifyInput] EXEC - LLM classification failed: {e}")
        
        return {"type": "topic_suggestion", "confidence": "high"}
    
    def post(self, shared, prep_res, exec_res):
        logger.info(f"[ClassifyInput] POST - Classification result: {exec_res}")
        shared["input_type"] = exec_res["type"]
        shared["classification_confidence"] = exec_res.get("confidence", "low")
        shared["classification_reason"] = exec_res.get("reason", "")
        
        # Return action based on classification
        return exec_res["type"]


class IngestQuery(Node):
    def prep(self, shared):
        logger.info("ğŸ” [IngestQuery] PREP - Äá»c role vÃ  input tá»« shared")
        role = shared.get("role", "")
        user_input = shared.get("input", "")
        logger.info(f"ğŸ” [IngestQuery] PREP - Role: {role}, Users Input : {user_input}")
        return role, user_input

    def exec(self, inputs):
        logger.info("ğŸ” [IngestQuery] EXEC - Xá»­ lÃ½ role vÃ  query")
        role, user_input = inputs
        result = {"role": role, "query": user_input.strip()}
        logger.info(f"ğŸ” [IngestQuery] EXEC - Processed: {result}")
        return result

    def post(self, shared, prep_res, exec_res):
        logger.info("ğŸ” [IngestQuery] POST - LÆ°u role vÃ  query vÃ o shared")
        shared["role"] = exec_res["role"]
        shared["query"] = exec_res["query"]
        logger.info(f"ğŸ” [IngestQuery] POST - Saved role: {exec_res['role']}, query: {exec_res['query'][:50]}...")
        return "default"

class RetrieveFromKB(Node):
    def prep(self, shared):
        logger.info("ğŸ“š [RetrieveFromKB] PREP - Äá»c query vÃ  keywords Ä‘á»ƒ retrieve")
        query = shared.get("query", "")
        keywords = shared.get("keywords", [])
        
        # Æ¯u tiÃªn dÃ¹ng keywords náº¿u cÃ³, náº¿u khÃ´ng thÃ¬ dÃ¹ng query gá»‘c
        search_term = " ".join(keywords) if keywords else query
        logger.info(f"ğŸ“š [RetrieveFromKB] PREP - Search Term: '{search_term[:100]}...'")
        return search_term

    def exec(self, search_term: str):
        logger.info("ğŸ“š [RetrieveFromKB] EXEC - Báº¯t Ä‘áº§u retrieve tá»« knowledge base")
        logger.info(f"ğŸ“š [RetrieveFromKB] EXEC - Query: {search_term}")
        results, score = retrieve(search_term, top_k=4)
        logger.info(f"ğŸ“š [RetrieveFromKB] EXEC - Retrieved results: {results} , best score: {score:.4f}")
        return results, score

    def post(self, shared, prep_res, exec_res):
        logger.info("ğŸ“š [RetrieveFromKB] POST - LÆ°u káº¿t quáº£ retrieve")
        results, score = exec_res
        shared["retrieved"] = results
        shared["retrieval_score"] = score
        shared["need_clarify"] = score < 0.15
        
        # Always continue to next node via default edge (ScoreDecisionNode)
        input_type = shared.get("input_type", "medical_question")
        logger.info(
            f"ğŸ“š [RetrieveFromKB] POST - Saved {len(results)} results, score: {score:.4f}, "
            f"input_type={input_type} -> routing via 'default' to ScoreDecision"
        )
        return "default" 

class GreetingResponse(Node):
    """Node xá»­ lÃ½ chÃ o há»i - set context vÃ  route Ä‘áº¿n topic suggestion"""
    def prep(self, shared):
        return shared.get("role", ""), shared.get("query", "")
    
    def exec(self, inputs):
        role, query = inputs
        return {"context_set": True, "role": role, "query": query}
    
    def post(self, shared, prep_res, exec_res):
        shared["explain"] = "ChÃ o báº¡n! TÃ´i lÃ  AI nha khoa. Náº¿u báº¡n cÃ³ cÃ¢u há»i hoáº·c cáº§n gá»£i Ã½ cÃ¡c cÃ¢u há»i liÃªn quan Ä‘áº¿n nha khoa vÃ  sá»©c khá»e rÄƒng miá»‡ng hoáº·c Ä‘Ã¡i thÃ¡o Ä‘Æ°á»ng, thÃ¬ cá»© nÃ³i tÃ´i nhÃ© ğŸ˜Š"
        return "default"


class ComposeAnswer(Node):
    def prep(self, shared):
        role = shared.get("role", "")
        query = shared.get("query", "")
        retrieved = shared.get("retrieved", [])
        score = shared.get("retrieval_score", 0.0)
        conversation_history = shared.get("conversation_history", [])
        logger.info(f"âœï¸ [ComposeAnswer] retrieved: {retrieved}")
        return (role, query, retrieved, score, conversation_history)

    def exec(self, inputs):
        role, query, retrieved,  score, conversation_history = inputs
        persona = get_persona_for(role)

        relevant_info_from_kb = get_most_relevant_QA(retrieved)
        prompt = PROMPT_COMPOSE_ANSWER.format(
            ai_role=persona['persona'],
            audience=persona['audience'],
            tone=persona['tone'],
            query=query,
            relevant_info_from_kb=relevant_info_from_kb,
            conversation_history = conversation_history
        )
        logger.info(f"âœï¸ [ComposeAnswer] EXEC - prompt: {prompt}")
        result = call_llm(prompt)
        logger.info(f"âœï¸ [ComposeAnswer] EXEC - LLM response received")
        result = parse_yaml_with_schema(result, required_fields=["explanation", "suggestion_questions"], field_types={"explanation": str, "suggestion_questions": list})
        logger.info(f"âœï¸ [ComposeAnswer] EXEC - result: {result}")

        if not result or  isinstance(result, str):
            logger.warning("[ComposeAnswer] EXEC - Invalid LLM response, using fallback")
            resp = "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ táº¡o cÃ¢u tráº£ lá»i phÃ¹ há»£p lÃºc nÃ y. Báº¡n Ä‘áº·t cÃ¢u há»i khÃ¡c Ä‘Æ°á»£c khÃ´ng? "
            return {"explain": resp, "suggestion_questions": [], "preformatted": True}
        
        return {"explain": result.get("explanation", ""), "suggestion_questions": result.get("suggestion_questions", []), "preformatted": True}


    def post(self, shared, prep_res, exec_res):
        logger.info("âœï¸ [ComposeAnswer] POST - LÆ°u answer object")
        shared["answer_obj"] = exec_res
        shared["explain"] = exec_res.get("explain", "")
        shared["suggestion_questions"] = exec_res.get("suggestion_questions", [])
        logger.info(f"âœï¸ [ComposeAnswer] POST - Answer keys: {list(exec_res.keys())}")
        logger.info(f"âœï¸ [ComposeAnswer] POST - Answer preview: {exec_res.get('explain')}")
        return "default"



class TopicSuggestResponse(Node):
    """Node xá»­ lÃ½ gá»£i Ã½ topic vá»›i template khÃ¡c nhau cho tá»«ng context"""
    
    def prep(self, shared):
        role = shared.get("role", "")
        query = shared.get("query", "")
        retrieved = shared.get("retrieved", [])
        logger.info(f"[TopicSuggestResponse] content retrieve: {retrieved}")
        context = shared.get("response_context", "default") 
        retrieval_score = shared.get("retrieval_score", 0.0)
        return role, query, retrieved, context, retrieval_score
    
    
    def exec(self, inputs):
        role, query, retrieved, context, retrieval_score = inputs
        result =    {
            "explain": "",
            "suggestion_questions" : [],
            "preformatted": True,
        }
        suggestion_questions = [q['cau_hoi'] for q in retrieve_random_by_role(role, amount=10)]
        # Handle low-score medical questions specifically
        if context == "medical_low_score":
            logger.info(f"[TopicSuggestResponse] EXEC - Handling low-score medical query: '{query}'")
            result["explain"] = "Hiá»‡n mÃ¬nh chÆ°a tÃ¬m Ä‘Æ°á»£c cÃ¢u tráº£ lá»i trong dá»¯ sáºµn cÃ³. Báº¡n thÃ´ng cáº£m nhÃ©!. MÃ¬nh cÃ³ cÃ¡c hÆ°á»›ng sau báº¡n cÃ³ thá»ƒ quan tÃ¢m nÃ¨."
        
        if context == "topic_suggestion":
            result["explain"] = "MÃ¬nh gá»£i Ã½ báº¡n cÃ¡c chá»§ Ä‘á» sau nhÃ©"
        
        result["suggestion_questions"] = suggestion_questions

        return result

    
    def post(self, shared, prep_res, exec_res):
        logger.info("[TopicSuggestResponse] POST - LÆ°u topic suggestion response")
        shared["answer_obj"] = exec_res
        shared["explain"] = exec_res.get("explain", "")
        shared["suggestion_questions"] = exec_res.get("suggestion_questions", [])  # Láº¥y 3 Ä‘áº§u lÃ m suggestions
        return "default"



class LogConversationNode(Node):
    """Node Ä‘á»ƒ log cuá»™c trÃ² chuyá»‡n user-bot vÃ o file"""
    
    def prep(self, shared):
        logger.info("[LogConversation] PREP - Chuáº©n bá»‹ log conversation")
        user_query = shared.get("query", "")
        bot_answer = shared.get("answer", "")
        return user_query, bot_answer
    
    def exec(self, inputs):
        logger.info("[LogConversation] EXEC - Logging conversation to file")
        user_query, bot_answer = inputs
        
        # Log the complete exchange
        log_conversation_exchange(user_query, bot_answer)
        
        return {"logged": True, "user_query": user_query, "bot_answer": bot_answer}
    
    def post(self, shared, prep_res, exec_res):
        logger.info(f"[LogConversation] POST - Logged conversation: user='{exec_res['user_query'][:50]}...' bot='{exec_res['bot_answer'][:50]}...'")
        shared["conversation_logged"] = exec_res["logged"]
        return "default"


class MainDecisionAgent(Node):
    """Main decision agent - chá»‰ phÃ¢n loáº¡i input vÃ  routing"""
    
    def prep(self, shared):
        logger.info("[MainDecision] PREP - Äá»c query Ä‘á»ƒ phÃ¢n loáº¡i")
        query = shared.get("query", "").strip()
        role = shared.get("role", "")
        return query, role
    
    def exec(self, inputs):
        query, role = inputs
        

        logger.info("[MainDecision] EXEC - Using LLM for classification")
        prompt = PROMPT_CLASSIFY_INPUT.format(query=query, role=role)
        
        try:
            resp = call_llm(prompt  )
            logger.info(f"[MainDecision] EXEC - resp: {resp}")
            result = parse_yaml_with_schema(
                resp,
                required_fields=["type"],
                optional_fields=["confidence", "reason", "keywords"],
                field_types={"type": str, "confidence": str, "reason": str, "keywords": list}
            )
            logger.info(f"[MainDecision] EXEC - result after parse: {result}")
            
            if result:
                logger.info(f"[MainDecision] EXEC - LLM classification: {result}")
                return result       
        except Exception as e:
            logger.warning(f"[MainDecision] EXEC - LLM classification failed: {e}")
        
        # Default fallback
        return {"type": "topic_suggestion", "confidence": "high", "keywords": []}
    
    def post(self, shared, prep_res, exec_res):
        logger.info(f"[MainDecision] POST - Classification result: {exec_res}")
        shared["input_type"] = exec_res["type"]
        shared["classification_confidence"] = exec_res.get("confidence", "low")
        shared["classification_reason"] = exec_res.get("reason", "")
        shared["keywords"] = exec_res.get("keywords", [])
        
        # Route based on classification
        input_type = exec_res["type"]
        
        if input_type == "medical_question":
            return "retrieve_kb"
        elif input_type == "topic_suggestion":
            return "retrieve_kb"
        elif input_type == "greeting":
            return "greeting"
        else:
            return "topic_suggestion"


class ScoreDecisionNode(Node):
    """Node quyáº¿t Ä‘á»‹nh dá»±a trÃªn retrieval score"""
    
    def prep(self, shared):
        logger.info("[ScoreDecision] PREP - Kiá»ƒm tra retrieval score")
        input_type = shared.get("input_type", "")
        retrieval_score = shared.get("retrieval_score", 0.0)
        return input_type, retrieval_score
    
    
    def exec(self, inputs):
        input_type, retrieval_score = inputs
        score_threshold = get_score_threshold()
        
        logger.info(f"[ScoreDecision] EXEC - Input: '{input_type}', Score: {retrieval_score:.4f}, Threshold: {score_threshold}")
        
        if input_type == "medical_question":
            if retrieval_score >= score_threshold:
                return {"action": "compose_answer", "context": "medical_high_score"}
            else:
                return {"action": "topic_suggest", "context": "medical_low_score"}
        
        return {"action": "topic_suggest", "context": "topic_suggestion"}
   
    def post(self, shared, prep_res, exec_res):
        shared["response_context"] = exec_res["context"]
        logger.info(f"[ScoreDecision] POST - Decision: {exec_res['action']}, Context: {exec_res['context']}")
        return exec_res["action"]
