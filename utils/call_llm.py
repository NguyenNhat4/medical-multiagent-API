import os
import logging
import re
from google import genai
from dotenv import load_dotenv
load_dotenv()
# Configure logging for this module
logger = logging.getLogger(__name__)
from google.genai import types

# Initialize client at module level to avoid overhead on each call
api_key = os.getenv("GEMINI_API_KEY", "")
if not api_key:
    logger.error(" GEMINI_API_KEY is not set")
    raise RuntimeError("GEMINI_API_KEY is not set")

client = genai.Client(api_key=api_key)


def estimate_tokens(text: str) -> int:
    """
    Estimate token count for Vietnamese and English text.
    Uses a simple heuristic: ~4 characters per token for Vietnamese, ~3.5 for English.
    This is a rough approximation for logging purposes.
    """
    if not text:
        return 0
    
    # Count Vietnamese characters (with diacritics)
    vietnamese_chars = len(re.findall(r'[Ã Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘Ä]', text))
    # Count total characters
    total_chars = len(text)
    
    # Rough estimation: Vietnamese text tends to have more tokens per character
    if vietnamese_chars > total_chars * 0.1:  # If >10% Vietnamese chars
        estimated_tokens = int(total_chars / 3.2)  # Vietnamese: ~3.2 chars per token
    else:
        estimated_tokens = int(total_chars / 3.8)  # English: ~3.8 chars per token
    
    return max(1, estimated_tokens)


# Learn more about calling the LLM: https://the-pocket.github.io/PocketFlow/utility_function/llm.html
def call_llm(prompt: str, fast_mode: bool = True) -> str:
    logger.info("ğŸ¤– Báº¯t Ä‘áº§u gá»i LLM...")
    
    # Log token count before API call
    estimated_tokens = estimate_tokens(prompt)
    logger.info(f"ğŸ“Š Estimated input tokens: {estimated_tokens} (prompt length: {len(prompt)} chars)")
    
    # Dynamic model selection based on fast_mode
    if fast_mode:
        model = os.getenv("GEMINI_MODEL", "gemini-1.5-flash-8b")  # Super fast
    else:
        model = os.getenv("GEMINI_MODEL_QUALITY", "gemini-2.5-flash")  # High quality
    
    logger.info(f"ğŸ¯ Using model: {model}")
    
    try:
        logger.info("â³ Äang gá»­i request Ä‘áº¿n Gemini...")
        response = client.models.generate_content(
        model=model,
        contents=prompt,
        config=types.GenerateContentConfig(
            temperature=0.5,
            top_p=0.9
        ),
    )
            
        response_text = response.text
        
        # Handle case where response.text is None
        if response_text is None:
            logger.warning("âš ï¸ Response text is None, checking response object...")
            logger.info(f"Response object: {response}")
            if hasattr(response, 'candidates') and response.candidates:
                logger.info(f"Candidates: {response.candidates}")
                if response.candidates[0].content and response.candidates[0].content.parts:
                    response_text = response.candidates[0].content.parts[0].text
                    logger.info(f"âœ… Extracted text from candidates: {len(response_text)} characters")
                else:
                    logger.error("âŒ No text found in candidates")
                    response_text = "Xin lá»—i, khÃ´ng thá»ƒ táº¡o response."
            else:
                logger.error("âŒ No candidates in response")
                response_text = "Xin lá»—i, khÃ´ng thá»ƒ táº¡o response."
        
        if response_text:
            output_tokens = estimate_tokens(response_text)
            logger.info(f"âœ… Nháº­n Ä‘Æ°á»£c response tá»« LLM: {len(response_text)} characters")
            logger.info(f"ğŸ“Š Estimated output tokens: {output_tokens}")
            logger.info(f"ğŸ“¤ Response preview: {response_text[:200]}...")
            logger.info(f"ğŸ” Full response for debugging: {response_text}")

        return response_text or "Xin lá»—i, khÃ´ng thá»ƒ táº¡o response."
        
    except Exception as e:
        logger.error(f"âŒ Lá»—i khi gá»i LLM: {str(e)}")
        raise

if __name__ == "__main__":
    test_prompt = "Hello, how are you?"
    print(call_llm(test_prompt))
